{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><b><font size=\"5\">Data Science Academy</font></b></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><b><font size=\"5\">Web Scraping e Análise de Dados</font></b></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><b><font size=\"5\">Lab 10 - Projeto Final</font></b></strong>\n",
    "\n",
    "<strong><b><font size=\"3\">Web Scraping, Processamento de Linguagem Natural, Modelagem de Tópicos e Análise de Sentimentos em Discursos Políticos</font></b></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagens/lab10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Problema\n",
    "\n",
    "Leia o manual em pdf onde você encontrou este Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonte de Dados\n",
    "\n",
    "As fontes de dados são os sites oficiais dos políticos que estamos extraindo os discursos:\n",
    "\n",
    "Prefeito da Cidade de New York: https://www1.nyc.gov\n",
    "\n",
    "Governador do Estado de New York: https://www.governor.ny.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os Pacotes Usados Neste Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook, se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacote para análise de sentimentos\n",
    "# https://pypi.org/project/vaderSentiment/\n",
    "!pip install -q vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Manipulação de dados\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import requests\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualização de dados\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "# Processamento de Linguagem Natural\n",
    "import spacy\n",
    "from spacy.symbols import amod\n",
    "from collections import Counter\n",
    "\n",
    "# Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Topic Modeling\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Análise de Sentimentos\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Definimos o parâmetro abaixo para evitar o erro:\n",
    "# RecursionError: maximum recursion depth exceeded\n",
    "# Ao gravar os resultados em disco\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer web scraping dos discursos do prefeito da cidade New York, Bill de Blasio, e do Governador do Estado de New York, Andrew Cuomo. \n",
    "\n",
    "Nosso objetivo é recuperar as transcrições de seus discursos oficiais em 2020. Como os sites de onde faremos web scraping são atualizados com discursos frequentemente, esse código produzirá um conjunto diferente de discursos (ou pode não funcionar) cada vez que for executado.\n",
    "\n",
    "E se algo mudar? Compreenda as mudanças e ajuste o código!!!\n",
    "\n",
    "Leia e estude o código abaixo. Seu aprendizado, também depende de você!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o chromedriver (semelhante a um dos Labs anteriores)\n",
    "# https://chromedriver.chromium.org/downloads\n",
    "chromedriver = \"./chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping dos Discursos do Prefeito Bill de Blasio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos Web Scraping das transcrições de discurso do prefeito Bill de Blasio. Começaremos raspando as URLs da página principal de transcrições e depois raspando o conteúdo de cada URL. Ou seja, \"Web Scraping do Web Scraping\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para armazenar a lista de urls\n",
    "lista_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop para criar a lista de urls para fazer web scraping \n",
    "for i in range(1, 41):\n",
    "    full_url = 'https://www1.nyc.gov/office-of-the-mayor/news.page' + '#page-' + str(i)\n",
    "    lista_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza a lista\n",
    "lista_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma função para fazer web scraping.  Altere a variável contador para capturar dados de mais ou menos urls (isso afeta o tempo total de execução do processo de web scraping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contador\n",
    "contador = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para web scraping\n",
    "def scraping_urls(urls):\n",
    "    \n",
    "    # Define o driver\n",
    "    driver = webdriver.Chrome(\"./chromedriver\")\n",
    "    \n",
    "    # Lista para o resultado\n",
    "    soup_list = []\n",
    "    \n",
    "    # Contador\n",
    "    count = 0\n",
    "    \n",
    "    # Loop pelas urls\n",
    "    for i in urls:\n",
    "        if count < contador:\n",
    "            driver.get(i)\n",
    "            driver.refresh()\n",
    "            time.sleep(5)\n",
    "            soup_list.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "        count += 1\n",
    "    driver.close()\n",
    "    return soup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping do código fonte de cada página dos principais discursos\n",
    "soups = scraping_urls(lista_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza\n",
    "soups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função abaixo vai extrair os links do codigo fonte das páginas html. Nesses links encontraremos o texto (transcrição) dos discursos. Usamos aqui expressões regulares em Python, conforme estudado em diversos cursos da DSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair os links de cada url do código fonte\n",
    "def extrai_links_prefeito(soup_object):\n",
    "    links_list = []\n",
    "    for s in soup_object:\n",
    "        links = s.find_all('a', {'href': re.compile(r'transcript')})\n",
    "        for i in links:\n",
    "            link1 = str(i).replace('\"', '')\n",
    "            if re.search('=(.+)>T', link1) is not None:\n",
    "                link = re.search('=(.+)>T', link1).group(1)\n",
    "            else:\n",
    "                continue\n",
    "            full_link = 'https://www1.nyc.gov' + link\n",
    "            links_list.append(full_link)\n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os links para cada página de transcrições de fala\n",
    "link_list = extrai_links_prefeito(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvamos os links em disco\n",
    "with open('dados/db_links_prefeito.pickle', 'wb') as to_write:\n",
    "    pickle.dump(link_list, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair o texto do discurso politico do código fonte\n",
    "def extrai_discurso_prefeito(urls):\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    doc_source = []\n",
    "    for i in urls:\n",
    "        driver.get(i)\n",
    "        time.sleep(5)\n",
    "        doc_source.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "    driver.close()\n",
    "    return doc_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai a transcrição de fala para cada discurso\n",
    "# Acompanhe a extração em tempo real na outra janela do seu navegador que será aberta!\n",
    "discursos = extrai_discurso_prefeito(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os discursos extraídos\n",
    "with open('dados/db_discurso_prefeito.pickle', 'wb') as to_write:\n",
    "    pickle.dump(discursos, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping dos Discursos do Governador Andrew Cuomo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O procedimento aqui é similar ao anterior, sendo que algumas páginas são diferentes e teremos que tratar isso em mais detalhes. Leia e estude o código abaixo. Seu aprendizado, também depende de você!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contador (altere para capturar mais ou menos dados)\n",
    "contador = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL com as páginas dos discursos\n",
    "url = 'https://www.governor.ny.gov/keywords/media'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para scraping ds urls\n",
    "def scraping_urls(urls):\n",
    "    \n",
    "    # Carrega o driver e extrai o códgo html\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    soup_list = []\n",
    "    driver.get(url)\n",
    "    driver.refresh()\n",
    "    time.sleep(5)\n",
    "    soup_list.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "    pages = list(range(2,9)) + ([4] * 12) \n",
    "    count = 0\n",
    "    \n",
    "    # Loop pelas páginas\n",
    "    for i in pages:\n",
    "        if count < contador:\n",
    "            path = '//*[@id=\"DataTables_Table_0_paginate\"]/span/a[' + str(i) + ']'\n",
    "            driver.find_element_by_xpath(path).click()\n",
    "            time.sleep(5)\n",
    "            soup_list.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "        count += 1\n",
    "    driver.close()\n",
    "    return soup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping do código fonte das páginas\n",
    "sources = scraping_urls(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair os links\n",
    "def extrai_links_gov(soup_object):\n",
    "    links_list = []\n",
    "    for s in soup_object:\n",
    "        links = s.find_all('a', {'href': re.compile(r'transcript')})\n",
    "        for i in links:\n",
    "            link1 = str(i).replace('\"', '')\n",
    "            if re.search('=(.+)>\\n', link1) is not None:\n",
    "                link = re.search('=(.+)>\\n', link1).group(1)\n",
    "            else:\n",
    "                continue\n",
    "            full_link = 'https://www.governor.ny.gov' + link\n",
    "            links_list.append(full_link)\n",
    "    return list(set(links_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair os links de cada código fonte\n",
    "link_list = extrai_links_gov(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os links de cada página\n",
    "with open('dados/db_links_governador.pickle', 'wb') as to_write:\n",
    "    pickle.dump(link_list, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair os discursos\n",
    "def extrai_discurso_gov(urls):\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    doc_source = []\n",
    "    for i in urls:\n",
    "        driver.get(i)\n",
    "        time.sleep(5)\n",
    "        doc_source.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "    driver.close()\n",
    "    return doc_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os discursos\n",
    "discursos = extrai_discurso_gov(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o discurso\n",
    "with open('dados/db_discurso_governador.pickle', 'wb') as to_write:\n",
    "    pickle.dump(discursos, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza de Dados\n",
    "\n",
    "Agora que extraímos a fonte de cada página de discursos, precisamos extrair o texto do discurso. \n",
    "\n",
    "Infelizmente, não existe um método abrangente e facilmente digerível para obter um discurso limpo e na maioria das vezes esse é um trabalho de tentativa e erro. Experimentamos alternativas até encontrar uma que funcione para o que precisamos, como o que você encontra abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o banco de dados\n",
    "with open('dados/db_links_prefeito.pickle', 'rb') as read_file:\n",
    "    db_links_prefeito = pickle.load(read_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o banco de dados\n",
    "with open('dados/db_discurso_prefeito.pickle', 'rb') as read_file:\n",
    "    db_discurso_prefeito = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair o texto\n",
    "def extrai_texto(source_object):\n",
    "    text_list = []\n",
    "    for s in source_object:\n",
    "        text = s.find_all('p')\n",
    "        text_list.append(text)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para o texto\n",
    "discurso_length = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop para extração do texto\n",
    "for i in extrai_texto(db_discurso_prefeito):\n",
    "    discurso_length.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte para array numpy\n",
    "discurso_length_array = np.array(discurso_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os discursos\n",
    "primeiros_discursos = extrai_texto(db_discurso_prefeito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estes são os índices que precisam ser re-extraídos com get_text2\n",
    "# Isso é necessário pois as páginas tem diferenças entre elas\n",
    "indices = list(np.argwhere(discurso_length_array == 1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para nova extração do texto\n",
    "def extrai_texto2(source_object):\n",
    "    text_list = []\n",
    "    for s in source_object:\n",
    "        text = s.find('p').parent\n",
    "        text_list.append(text)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop para extrair discursos com texto mais claro\n",
    "melhores_discursos = []\n",
    "for i in indices:\n",
    "    melhores_discursos.append(extrai_texto2(db_discurso_prefeito[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir os primeiros_discursos por melhores_discursos\n",
    "for (indices, melhores_discursos) in zip(indices, melhores_discursos):\n",
    "    primeiros_discursos[indices] = melhores_discursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover tags html\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para limpeza dos dados\n",
    "def limpa_dados(transcript_list, link_list):\n",
    "    date = []\n",
    "    text = []\n",
    "    for i in transcript_list:\n",
    "        cleaned = remove_html_tags(str(i))\n",
    "        if re.search('\\[\\\\n(.+)\\\\n(.+)\\]', cleaned) is not None and re.search('\\[\\\\n(.+)\\\\n(.+)\\]', cleaned) is not None:\n",
    "            date_clean = re.search('\\[\\\\n(.+)\\\\n(.+)\\]', cleaned).group(1)\n",
    "            date.append(date_clean)\n",
    "            text_clean = re.search('\\[\\\\n(.+)\\\\n(.+)\\]', cleaned).group(2)\n",
    "            text.append(text_clean)\n",
    "        else:\n",
    "            date_clean = re.search('\\\\n\\\\n(.+20.{2})\\\\n(.+)\\\\n\\\\ufeff', cleaned).group(1)\n",
    "            date.append(date_clean)\n",
    "            text_clean = re.search('\\\\n\\\\n(.+20.{2})\\\\n(.+)\\\\n\\\\ufeff', cleaned).group(2)\n",
    "            text.append(text_clean)\n",
    "    date = pd.to_datetime(date)\n",
    "    df = pd.DataFrame([date, link_list, text]).T\n",
    "    df.columns = ['date', 'link', 'text']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza\n",
    "dados_limpos = limpa_dados(primeiros_discursos, db_links_prefeito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair o discurso usando padrões de expressões regulares\n",
    "def extrai_texto_discurso(transcript):\n",
    "    if len(re.findall('sio:([^:]+)|yor:([^:]+)', str(transcript))) > 0:\n",
    "        return str(re.findall('sio:([^:]+)|yor:([^:]+)', str(transcript)))\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padrão para remover\n",
    "toremove = \"'\\\\,\\\"\\[\\(\\]\\)-–\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções lambda (anônimas) para extrair caracteres e padrões indesejados dos dados\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(toremove), '', x.lower())\n",
    "remove_xa0 = lambda x: x.replace('xa0', '')\n",
    "remove_space = lambda x: x.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a limpeza\n",
    "dados_limpos['monologue'] = dados_limpos['text'].map(extrai_texto_discurso).map(punc_lower).map(remove_xa0).map(remove_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluímos uma coluna indicando que este é um discurso do prefeito deBlasio\n",
    "dados_limpos.insert(0, 'speaker', 'de blasio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvamos os discursos agora limpos\n",
    "with open('dados/db_discurso_prefeito_limpo.pickle', 'wb') as to_write:\n",
    "     pickle.dump(dados_limpos, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos repetir o processo para os discursos do Governador, com alguns pequenos ajustes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o banco de dados\n",
    "with open('dados/db_links_governador.pickle', 'rb') as read_file:\n",
    "    db_links_governador = pickle.load(read_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o banco de dados\n",
    "with open('dados/db_discurso_governador.pickle', 'rb') as read_file:\n",
    "    db_discurso_governador = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para obter a data (usaremos isso mais tarde)\n",
    "def get_date(source_object):\n",
    "    date_list = []\n",
    "    for i in source_object:\n",
    "        date = i.find('div', class_=\"published-date\").text\n",
    "        date_clean = re.search('\\\\n\\\\n(.+20.{2})', date).group(1).strip()\n",
    "        date_list.append(date_clean)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai a data\n",
    "cuomo_date = get_date(db_discurso_governador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair o texto\n",
    "def get_text(source_object):\n",
    "    text_list = []\n",
    "    for s in source_object:\n",
    "        text = s.find('div', class_='field field--name-field-body field--type-text-long field--label-hidden')\n",
    "        text_list.append(text)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai o texto com uma primeira passada pelos dados\n",
    "cuomo_step1 = get_text(db_discurso_governador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para limpeza dos dados\n",
    "def clean_cuomo(transcript_list):\n",
    "    clean_transcripts = []\n",
    "    for idx, i in enumerate(transcript_list):\n",
    "        cleaned = remove_html_tags(str(i))\n",
    "        if re.search('below:(.+)', cleaned) is not None:\n",
    "            text_clean = re.search('below:(.+)', cleaned).group(1)\n",
    "            clean_transcripts.append(text_clean)\n",
    "        elif re.search('here.(.+)', cleaned) is not None:\n",
    "            text_clean = re.search('here.(.+)', cleaned).group(1)\n",
    "            clean_transcripts.append(text_clean)\n",
    "    return clean_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa os dados\n",
    "cuomo_step2 = clean_cuomo(cuomo_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos visualizar o que extraímos\n",
    "test_len = []\n",
    "for i in cuomo_step2:\n",
    "    test_len.append(len(i))\n",
    "test_len_array = np.array(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de texto extraído\n",
    "cuomo_step2[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de texto extraído\n",
    "cuomo_step2[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para liberar memória, deletamos objetos que não precisamos mais\n",
    "delete_index = test_len_array.argsort()[0]\n",
    "del cuomo_step2[delete_index]\n",
    "del db_links_governador[delete_index]\n",
    "del cuomo_date[delete_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertemos a data para o tipo datetime\n",
    "cuomo_date = pd.to_datetime(cuomo_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos o dataframe com dados limpos\n",
    "cuomo_clean = pd.DataFrame([cuomo_date, db_links_governador, cuomo_step2]).T\n",
    "cuomo_clean.columns = ['date', 'links', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza\n",
    "cuomo_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvamos os dicusros\n",
    "with open('dados/db_discurso_governador_limpo.pickle', 'wb') as to_write:\n",
    "    pickle.dump(cuomo_clean, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos três funções diferentes com expressões regulares diferentes para extrair apenas as partes do discurso em que o Governador estava falando (não queremos comentários de outras pessoas). Para nossa sorte, temos essas informação na página de cada discurso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair discurso\n",
    "def extrai_discurso_gov1(transcript):\n",
    "    if len(re.findall('Cuomo:([^:]+)', str(transcript))) > 0:\n",
    "        return str(re.findall('Cuomo:([^:]+)', str(transcript)))\n",
    "    else:\n",
    "        return str(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair discurso\n",
    "def extrai_discurso_gov2(transcript):\n",
    "    if len(re.findall('Cuomo:(.*?)[A-Z][a-z]+:', str(transcript))) > 0:\n",
    "        return str(re.findall('Cuomo:(.*?)[A-Z][a-z]+:', str(transcript)))\n",
    "    else:\n",
    "        return str(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair discurso\n",
    "def extrai_discurso_gov3(transcript):\n",
    "    if len(re.findall('^(.*?)[A-Z][a-z]+:', str(transcript))) > 0:\n",
    "        return str(re.findall('^(.*?)[A-Z][a-z]+:', str(transcript)))\n",
    "    else:\n",
    "        return str(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza dos dados\n",
    "cuomo_clean['monologue'] = cuomo_clean['text'].map(extrai_discurso_gov1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza dos dados\n",
    "cuomo_clean['monologue2'] = cuomo_clean['text'].map(extrai_discurso_gov2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a terceira função apenas se os dois primeiros não capturarem a transcrição correta\n",
    "def extract_m3(col1, col2):\n",
    "    if (len(col1)-len(col2))/len(col2) > 10:\n",
    "        return extrai_discurso_gov3(col1)\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza dos dados\n",
    "cuomo_clean['monologue3'] = cuomo_clean.apply(lambda x: extract_m3(x.text, x.monologue), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para comparação\n",
    "def compara(col1, col2):\n",
    "    if (len(col1) - len(col2)) / len(col2) > 1:\n",
    "        return col1\n",
    "    else:\n",
    "        return col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto final com duas extrações\n",
    "cuomo_clean['final_text'] = cuomo_clean.apply(lambda x: compara(x.monologue, x.monologue2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto final com a terceira extração\n",
    "cuomo_clean['final_text2'] = cuomo_clean.monologue3 + cuomo_clean.final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removemos caracteres indesejados\n",
    "remove_sxa0 = lambda x: x.replace('\\xa0', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza final\n",
    "cuomo_clean['final_clean'] = cuomo_clean['final_text2'].map(punc_lower).map(remove_xa0).map(remove_sxa0).map(remove_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza final\n",
    "cuomo_final = cuomo_clean.loc[:, ['date', 'links', 'text', 'final_clean']]\n",
    "cuomo_final.columns = ['date', 'link', 'text', 'monologue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluímos uma coluna indicando que este é um discurso do Governador Cuomo\n",
    "cuomo_final.insert(0, 'speaker', 'cuomo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza\n",
    "cuomo_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva em disco\n",
    "with open('dados/db_discurso_governador_limpo_final.pickle', 'wb') as to_write:\n",
    "    pickle.dump(cuomo_final, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrimos os bancos de dados já limpos para concatená-los no mesmo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dados/db_discurso_prefeito_limpo.pickle','rb') as read_file:\n",
    "    deblasio = pickle.load(read_file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dados/db_discurso_governador_limpo_final.pickle','rb') as read_file:\n",
    "    cuomo = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatena os databases dos discursos\n",
    "db_final = pd.concat([cuomo, deblasio], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos a proporção de discursos para cada político\n",
    "db_final['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava o database final\n",
    "with open('dados/db_final.pickle', 'wb') as to_write:\n",
    "     pickle.dump(db_final, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se necessário retorne e colete mais dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3 - Topic Modeling\n",
    "\n",
    "Modelagem de Tópicos é uma forma de mineração de texto, uma forma de identificar padrões. Construindo um corpus e executando uma ferramenta que gera grupos de palavras a respeito do corpus distribuídas em “tópicos”. \n",
    "\n",
    "Modelagem de tópicos é um método para achar e traçar clusters de palavras (chamado “tópicos” de forma abreviada) em grandes conjuntos de texto.\n",
    "\n",
    "Aqui tem uma definição completa sobre o tema:\n",
    "\n",
    "http://journalofdigitalhumanities.org/2-1/topic-modeling-a-basic-introduction-by-megan-r-brett/#topic-modeling-a-basic-introduction-by-megan-r-brett-n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download do modelo de linguagem do spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para usar o spaCy, precisamos referenciar o modelo de linguagem\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamos o banco de dados\n",
    "with open('dados/db_final.pickle','rb') as read_file:\n",
    "    db_final = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o Processamento de Linguagem Natural criamos alguns padrões para extração no texto dos discursos\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', '', x)\n",
    "punc = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "remove_space = lambda x: x.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos as funções anteriores ao database final\n",
    "db_final['for_spacy'] = (db_final['monologue']\n",
    "                         .map(alphanumeric)\n",
    "                         .map(punc)\n",
    "                         .map(remove_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora aplicamos o modelo de linguagem (sp) do SpaCy\n",
    "db_final['spacy_monologue'] = db_final['for_spacy'].map(lambda x: sp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza\n",
    "db_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematização é o processo de extrair o lema de cada palavra, tarefa fundamental em PLN. Não precisamos da palavra inteira, somente do seu lema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematização\n",
    "db_final['lemmatized'] = (db_final['spacy_monologue']\n",
    "                          .map(lambda x: [' '.join(word.lemma_ \n",
    "                                                   if word.lemma_ != '-PRON-' \n",
    "                                                   else word.text for word in x)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza\n",
    "db_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui nós criamos uma lista de stop words palaras que não são relevantes ou representam os nomes dos entrevistadores, que também não são necessárias para esta análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de stop words\n",
    "lista_stop_words = (text\n",
    "                    .ENGLISH_STOP_WORDS\n",
    "                    .union(['lehrer', 'brian', 'darden', 'moderator', 'alan', 'howard', 'wolf', 'blitzer', \n",
    "                            'errol', 'louis', 'alisyn', 'chris', 'camerota', 'dan', 'mannarino','john', 'berman', \n",
    "                            'savannah', 'guthrie', 'hoda']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando max_df para 0,5 e min_df = 2, porque estes forneceram os melhores tópicos\n",
    "cv1 = CountVectorizer(stop_words = lista_stop_words, max_df = 0.5, min_df = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a matriz de documentos, que basicamente contém palavras em representações numéricas\n",
    "docterm_matrix = cv1.fit_transform(db_final.loc[:, 'lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "doc_label = ['Document' + str(t) for t in range(len(db_final.loc[:, 'lemmatized']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz pronta\n",
    "pd.DataFrame(docterm_matrix.toarray(), index = doc_label, columns = cv1.get_feature_names()).iloc[:10, 630:650]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos Non-Negative Matrix Factorization (NMF) para preencher a matriz com os tópicos por documento.\n",
    "\n",
    "O código abaixo usa o NMF para 12 tópicos, mas tentamos vários hiperparâmetros diferentes (incluindo a configuração de max_df e min_df no count_vectorizer) antes de escolher o 12, que fornecia os tópicos mais claros e distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria e treina o modelo\n",
    "nmf_cv = NMF(12)\n",
    "nmf_topics1 = nmf_cv.fit_transform(docterm_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe de tópicos\n",
    "topicword_cv1 = pd.DataFrame(nmf_cv.components_.round(3),\n",
    "                            index = ['topic0', 'topic1', 'topic2', 'topic3',\n",
    "                                     'topic4', 'topic5', 'topic6', 'topic7',\n",
    "                                     'topic8', 'topic9', 'topic10', 'topic11'],\n",
    "                            columns = cv1.get_feature_names()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos um dataframe com os 12 tópicos nas linhas e os termos nas colunas. Os valores no dataframe descrevem como o termo se relaciona com o tópico, com valores mais altos indicando um relacionamento mais forte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz\n",
    "topicword_cv1.iloc[:, 1790:1820]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para encontrar as palavras mais importantes por tópico\n",
    "def top_words_per_topic(model, terms, topic_names = None):\n",
    "    for ix, topic in enumerate(nmf_cv.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTópico \", ix)\n",
    "        else:\n",
    "            print(\"\\nTópico '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([cv1.get_feature_names()[i] for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos os speakers (políticos)\n",
    "db_final_docs_topics = db_final.loc[:, ['speaker']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza\n",
    "db_final_docs_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos os tópicos mais relevantes\n",
    "db_final_docs_topics['topics12'] = nmf_topics1.argmax(axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo em vista esses tópicos, vamos entender como os palestrantes se envolveram com cada tópico. Para isso, analisamos as contagens gerais por tópico e depois dividimos por orador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tópicos por político\n",
    "speaker_topics = pd.DataFrame(db_final_docs_topics.groupby(['topics12']).speaker.value_counts())\n",
    "speaker_topics.columns = ['count']\n",
    "speaker_topics.reset_index()\n",
    "speaker_topics_pivot = speaker_topics.reset_index().pivot_table(index = 'topics12', columns = 'speaker', values = 'count', fill_value = 0).reset_index(drop = True)\n",
    "speaker_topics_pivot['total'] = speaker_topics_pivot['cuomo'] + speaker_topics_pivot['de blasio']\n",
    "speaker_topics_pivot.sort_values('total', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos o resultado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize = (14, 8))\n",
    "names = list(set(speaker_topics_pivot.index))\n",
    "plt.title('Tópicos Mais Abordados nos Discursos')\n",
    "plt.xlabel('Total de Documentos por Tópico')\n",
    "plt.xlim(0, 80)\n",
    "plt.barh(names, speaker_topics_pivot['total'], color = 'magenta', edgecolor = 'white', height = barWidth)\n",
    "plt.yticks(range(0, 12), ['Neighborhood Impact', 'Covid Mechanics', 'Healthcare', 'Education/School', \n",
    "                         'Neighborhood Resilience', 'Homelessness', 'DOH Communication', 'New Yorkers',\n",
    "                         'Hate Crime', 'Hospital Needs', 'Hospital Status', 'Reopening Metrics'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tópico que aparece com mais frequência refere-se a métricas para reabertura. Isso inclui termos como taxa, vírus, reabertura, infecção, negócios, infecção e hospitalização. Isso faz sentido, já que a cidade de New York está em processo de reabertura da economia no momento que executamos este Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize = (14, 8))\n",
    "names = list(set(speaker_topics_pivot.index))\n",
    "plt.title('Politicos Cobrindo Diferentes Tópicos')\n",
    "plt.xlabel('Total de Documentos por Tópico')\n",
    "plt.xlim(0, 80)\n",
    "plt.barh(names, speaker_topics_pivot['cuomo'], color = '#77a9cf', edgecolor = 'white', height = barWidth, label = 'Cuomo')\n",
    "plt.barh(names, speaker_topics_pivot['de blasio'], left = speaker_topics_pivot['cuomo'], color = '#df8a62', edgecolor = 'white', height = barWidth, label = 'de Blasio')\n",
    "plt.yticks(range(0, 12), ['Neighborhood Impact', 'Covid Mechanics', 'Healthcare', 'Education/School', \n",
    "                         'Neighborhood Resilience', 'Homelessness', 'DOH Communication', 'New Yorkers',\n",
    "                         'Hate Crime', 'Hospital Needs', 'Hospital Status', 'Reopening Metrics'])\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reabertura é puramente um tópico de Cuomo. Cuomo também iniciou muitos de seus discursos, fornecendo estatísticas relevantes para a reabertura. O Prefeito de Blasio domina o tópico das necessidades hospitalares, enquanto Cuomo discute o status dos hospitais. Eles descrevem coisas diferentes, nas quais os discursos de Cuomo tendem a envolver atualizações sobre ventiladores, camas e equipamentos disponíveis, e de Blasio aborda o que os hospitais de New York estão perdendo: pessoal, suprimentos e ventiladores. Ambos estão cobrindo hospitais de maneiras ligeiramente diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Prefeito de Blasio se concentra no atendimento público universal e disponível, enquanto Cuomo se concentra no tratamento apropriado e intensivo. A análise de partes do discurso destaca as raízes progressivas de Blasio, com seu foco nos nova-iorquinos da classe trabalhadora e a necessidade de cuidados de saúde públicos universais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4 - Análise de Sentimentos\n",
    "\n",
    "Esta é a última parte do trabalho.\n",
    "\n",
    "Usaremos a função SentimentIntensityAnalyzer e alimentamos os discursos para recuperar as pontuações de sentimentos. O pacote VADER retorna uma pontuação negativa, neutra, positiva e composta, que pode ser plotada ao longo do tempo para entender como os sentimentos mudam ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o dataset\n",
    "with open('dados/db_final.pickle','rb') as read_file:\n",
    "    db_final = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o analisador de sentimentos\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de scores\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula os scores de sentimentos\n",
    "for i in db_final['monologue']:\n",
    "    scores.append(analyzer.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos o resultado \n",
    "db_final_docs_sentiment = pd.concat([db_final.loc[:, ['speaker', 'date']], pd.DataFrame(scores)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora calculamos médias de sentimentos para cada orador, observando a pontuação composta e a positiva. É como criar uam janela deslizante em análise de série temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide os dados por político\n",
    "cuomo_roll = db_final_docs_sentiment[db_final_docs_sentiment.speaker == 'cuomo'].sort_values(by = 'date')\n",
    "de_blasio_roll = db_final_docs_sentiment[db_final_docs_sentiment.speaker == 'de blasio'].sort_values(by = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias móveis do Governador\n",
    "cuomo_roll['cp_roll_avg'] = cuomo_roll.compound.rolling(window = 7).mean()\n",
    "cuomo_roll['pos_roll_avg'] = cuomo_roll.pos.rolling(window = 7).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias móveis do Prefeito\n",
    "de_blasio_roll['cp_roll_avg'] = de_blasio_roll.compound.rolling(window = 7).mean()\n",
    "de_blasio_roll['pos_roll_avg'] = de_blasio_roll.pos.rolling(window = 7).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinamos as médias móveis em um único dataframe\n",
    "combined_roll = pd.concat([cuomo_roll, de_blasio_roll]).sort_values('date').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza\n",
    "combined_roll.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito. Temos uma série temporal de discursos. Várias análises e previsões poderiam ser feitas aqui e deixaremos isso com você. Vamos apenas plotar o resultado final.\n",
    "\n",
    "Lembre-se que alunos das Formações DSA tem acesso ao curso bônus gratuito de Análise de Séries Temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot do Score Composto\n",
    "fig = plt.figure(figsize = (15, 8))\n",
    "ax = plt.axes()\n",
    "sns.lineplot(x = 'date', y = 'cp_roll_avg', hue = 'speaker', data = combined_roll)\n",
    "plt.title('Score Composto 7-Dias Rolling Average')\n",
    "plt.xlabel('Data do Discurso')\n",
    "plt.ylabel('Score Composto')\n",
    "plt.ylim((-0.6, 1.1))\n",
    "plt.axhline(y = 0, color = '#77a9cf', linestyle = ':')\n",
    "plt.axvline(x = datetime.date(2020, 7, 1), color = 'black', linestyle = 'dashed')\n",
    "plt.xticks(rotation = 30)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olhando para o gráfico da pontuação composta, percebemos que o Governador Cuomo teve pontuação baixa (indicando discurso com tom mais negativo) em Junho/2020, melhorando sua pontuação em Julho. O Prefeito de Blasio manteve seus discursos com tom positivo de forma quase constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot do Score Positivo\n",
    "fig = plt.figure(figsize = (15, 8))\n",
    "ax = plt.axes()\n",
    "sns.lineplot(x = 'date', y = 'pos_roll_avg', hue = 'speaker', data = combined_roll)\n",
    "plt.title('Score Positivo 7-Dias Rolling Average')\n",
    "plt.xlabel('Data do Discurso')\n",
    "plt.ylabel('Score Positivo')\n",
    "plt.axvline(x = datetime.date(2020, 7, 1), color = 'black', linestyle = 'dashed')\n",
    "plt.xticks(rotation = 30);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olhando para o gráfico de pontuação positiva, a média móvel de 7 dias também é mais alta para o Prefeito de Blasio do que para o Governador Cuomo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse trabalho ainda permite diversas análises, modelagem preditiva ou mesmo comparações com outros eventos. Experimente modificar o projeto e continuar o processo de análise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207.986px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
